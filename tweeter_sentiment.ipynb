{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a5 - Tweeter Sentiment\n",
    "\n",
    "In this assignment you will write a program to perform simple [sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis) of Twitter data&mdash;that is, determining the \"attitude\" or \"emotion\" (e.g., how \"positive\", \"negative\", \"joyful\", etc) of tweets made by a particular Twitter user. Sentiment analysis is a fascinating field: researchers have shown that the \"mood\" of Twitter communication [reflects biological rhythms](http://www.nytimes.com/2011/09/30/science/30twitter.html) and can even be used to [predict the stock market](http://arxiv.org/pdf/1010.3003&embedded=true). The particular analysis you'll be performing is inspired by an investigation of [personal vs. organizational tweets](http://varianceexplained.org/r/trump-tweets/) (which has become less amusing over time).\n",
    "\n",
    "You will be implementing a Python program that performs this analysis on **real data** taken directly from a Twitter user's timeline. In the end, your script will produce output similar to the following:\n",
    "\n",
    "```\n",
    "EMOTION       % WORDS  EXAMPLE WORDS                     HASHTAGS\n",
    "positive      6.16%    learn, faculty, happy             #accesstoinfoday, #indigenouspeoplesday, #idealistfair\n",
    "trust         3.08%    school, faculty, happy            #indigenouspeoplesday, #diversity\n",
    "anticipation  2.53%    happy, top, ready                 #indigenouspeoplesday, #informatics, #info340\n",
    "joy           1.76%    happy, peace, deal                #indigenouspeoplesday, #accesstoinfoday\n",
    "surprise      0.99%    deal, award, surprised            #suzzallolibrary, #nobrainer\n",
    "negative      0.88%    fall, rejection, outstanding        \n",
    "sadness       0.55%    fall, rejection, problem            \n",
    "disgust       0.44%    rejection, weird, finally           \n",
    "fear          0.44%    rejection, surprise, problem        \n",
    "anger         0.33%    rejection, disaster, involvement  #mlis\n",
    "```\n",
    "\n",
    "Fill in the below code cells as specified. Note that cells may utilize variables and functions defined in previous cells; we should be able to use the `Kernal > Restart & Clear All` menu item followed by `Cell > Run All` to execute your entire notebook and see the correct output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "You'll be working with two different pieces of data for this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you'll be loading tweet data taken directly from [Twitter's API](https://developer.twitter.com/en/docs/tweets/timelines/api-reference/get-statuses-user_timeline). You can find an example of this tweet data in the **`uw_ischool_sample.py`** file inside the `data/` folder. The below cell will import this data as a variable `SAMPLE_TWEETS` from the provided _module_ file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import from uw_ischool_sample file in the `data/` package (folder)\n",
    "from data.uw_ischool_sample import SAMPLE_TWEETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is represented as one giant **list of dictionaries**: the **list** contains a sequence of **dictionaries**, where each dictionary represents a tweet. Each dictionary contains many different _value_, some of which themselves may be dictionaries.\n",
    "\n",
    "Print out the first three elements from the `SAMPLE_TWEETS` list to see what information can be found. The most relevant value is the `\"text\"` of the tweet.\n",
    "- The Twitter API actually provides a lot more information about each tweet; I've stripped it down to only the most important properties for readability. Each dictionary is a proper subset of the full data you'd get from Twitter.\n",
    "- Because of the source of the sentiment data, your analysis will be biased and only support English-language speakers. Nevertheless, Twitter is an international community so you may encounter non-English characters and words. You'll be working with real-world data and it will be messy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'created_at': 'Mon Oct 10 18:39:51 +0000 2016', 'retweet_count': 9, 'entities': {'hashtags': [{'indices': [20, 41], 'text': 'IndigenousPeoplesDay'}]}, 'user': {'screen_name': 'UW_iSchool'}, 'text': 'RT @UWAPress: Happy #IndigenousPeoplesDay https://t.co/YmU9e9lj7v'}, {'created_at': 'Mon Oct 10 18:00:00 +0000 2016', 'retweet_count': 0, 'entities': {'hashtags': [{'indices': [16, 29], 'text': 'IdealistFair'}]}, 'user': {'screen_name': 'UW_iSchool'}, 'text': \"We'll be at the #IdealistFair this evening on the Seattle U. campus. Come and learn about our graduate programs: https://t.co/et1HrQshmr\"}]\n"
     ]
    }
   ],
   "source": [
    "print(SAMPLE_TWEETS[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second piece of data you'll be working with is a set of **word-sentiments**&mdash;a list of English-language words and what emotions (e.g., \"joy\", \"anger\") [are associated with them](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm).\n",
    "\n",
    "- The [`nltk`](https://github.com/nltk/nltk/wiki/Sentiment-Analysis) library you used in the last assignment does support sentiment analysis. However, for practice and extendability you'll be doing a more \"manual\" analysis using the provided data file for this assignment.\n",
    "\n",
    "`import` the word sentiments as a variable **`SENTIMENTS`** from the **`data.sentiments_nrc`** module. You should also import the `EMOTIONS` variable provided by the same module: this is a _list_ of possible emotions. You can inspect the variables (e.g., print them out) to confirm that you have imported them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data.sentiments_nrc import SENTIMENTS\n",
    "from data.sentiments_nrc import EMOTIONS\n",
    "#print(SENTIMENTS)\n",
    "#print(EMOTIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Sentiment\n",
    "All of the sentiment analysis is based on the individual _words_ in the text. Thus you will need to will determine which words in a tweet have which sentiments.\n",
    "\n",
    "- Note that the assignment explicitly does _not_ tell you what to name functions, what arguments they should take or values they should return: your task is to determine appropriate functions and arguments from the (guided) requirements! Use multiple functions for clarity, give them all informative names, and include a **doc string** to explain what it does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that take a tweet's text (a string) and split it up into a list of individual words.\n",
    "\n",
    "- To support future assignments, you should **not** use the `nltk` library to tokenize words. Instead, your analysis should split up the text using the [regular expression](https://www.regular-expressions.info/) **`\"\\W+\"`** as a separator to \"split up\" the words by (rather than just a blank space). You can do this by using the [re.split()](https://docs.python.org/3/library/re.html#re.split) function (from the `re` module). This separator will cause your spitting to exclude punctuation and provide a reasonable (but not perfect!) list of words to consider. \n",
    "\n",
    "- All of the words in the sentiment dictionary are _lower case_, so you'll need to **map** your resulting words to be lower case. You will also need to **filter** out any words that have 1 letter or fewer. Use a **list comprehension** to do this.\n",
    "\n",
    "The string `\"Amazingly, I prefer a #rainy day to #sunshine.\"` should produce a list with 6 lower-case words in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amazingly', 'prefer', 'rainy', 'day', 'to', 'sunshine']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def text_to_word(string):\n",
    "    word_list = re.split(r'\\W+', string)\n",
    "    word_list = list(map(lambda x:x.lower(),word_list))\n",
    "    word_list = list(filter(lambda x: len(x)>1,word_list))\n",
    "    return word_list\n",
    "text_to_word(\"Amazingly, I prefer a #rainy day to #sunshine.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that **filters** a list of the words to get only those words that contain a specific emotion. Use a **list comprehension** to do this.\n",
    "- You can determine whether a word has a particular emotion by looking it up in the imported `SENTIMENTS` variable. Use the word as the \"key\" to find the dictionary of emotions for that word, and then use the emotion as the key to _that_ dictionary to determine if the word has it. \n",
    "    \n",
    "    Do not use the `in` operator or a loop to search the list of keys one by one. Instead, use bracket notation or the `get()` method to \"look up\" a key directly (this is more efficient and is why the emotions are nested dictionaries in the first place).\n",
    "    \n",
    "For testing, the `\"positive\"` words extracted from `\"Amazingly, I prefer a #rainy day to #sunshine.\"` are `[\"amazingly\", \"prefer\", \"sunshine\"]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amazingly', 'prefer', 'sunshine']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_to_emotion(string, emotion):\n",
    "    emotion_list = []\n",
    "    for word in text_to_word(string):\n",
    "        # to solve the error of \"'NoneType' object has no attribute 'get'\"\n",
    "        if SENTIMENTS.get(word,None)!= None:\n",
    "            if SENTIMENTS.get(word,None).get(emotion) == 1:\n",
    "                emotion_list.append(word)\n",
    "    return emotion_list\n",
    "word_to_emotion(\"Amazingly, I prefer a #rainy day to #sunshine.\",'positive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that determines which words from a list have _each_ emotion (i.e., the \"emotional\" words). For example, the words extracted from `\"Amazingly, I prefer a #rainy day to #sunshine.\"` should produce a dictionary that looks like:\n",
    "\n",
    "```\n",
    "{\n",
    " 'anger': [],\n",
    " 'anticipation': [],\n",
    " 'disgust': [],\n",
    " 'fear': [],\n",
    " 'joy': ['amazingly', 'sunshine'],\n",
    " 'negative': [],\n",
    " 'positive': ['amazingly', 'prefer', 'sunshine'],\n",
    " 'sadness': ['rainy'],\n",
    " 'surprise': ['amazingly'],\n",
    " 'trust': ['prefer']\n",
    "}\n",
    "```\n",
    "    \n",
    "(Note the empty lists for emotions that have no matching words).\n",
    "    \n",
    "- You can use the imported `EMOTIONS` variable to have a list of emotions to iterate through.\n",
    "- Use the function you defined in the previous step to help you out!\n",
    "- Using a [dictionary comprehension](https://www.smallsurething.com/list-dict-and-set-comprehensions-by-example/) is a nice way to do this, but is not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anger': [],\n",
       " 'anticipation': [],\n",
       " 'disgust': [],\n",
       " 'fear': [],\n",
       " 'joy': ['amazingly', 'sunshine'],\n",
       " 'negative': [],\n",
       " 'positive': ['amazingly', 'prefer', 'sunshine'],\n",
       " 'sadness': ['rainy'],\n",
       " 'surprise': ['amazingly'],\n",
       " 'trust': ['prefer']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def emotions_in_word(string):\n",
    "    dict = {}\n",
    "    for emo in EMOTIONS:\n",
    "        dict[emo] = word_to_emotion(string, emo)\n",
    "    return dict\n",
    "emotions_in_word(\"Amazingly, I prefer a #rainy day to #sunshine.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that gets a list of the \"most common\" words in a list: that is a new list containing each word in the original list, in descending order by how many times that word appears in the orignal list.\n",
    "\n",
    "- You can determine the frequency (number of occurrences) of a word with a similar process to what you did with digits in the last assignment.\n",
    "- You should use the `sorted()` function to [sort](https://wiki.python.org/moin/HowTo/Sorting) the individual words. This function take a **`key`** argument which should be passed a [_callback function_](https://wiki.python.org/moin/HowTo/Sorting#Key_Functions) that can return a \"transformed\" value that you wish to sort by (e.g., which element in a tuple). An anonymous lambda function works well for this.\n",
    "\n",
    "You can test this function with any list of \"words\" with repeated entries: `['a','b','c','c','c','a']` for example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('c', 3), ('a', 2), ('b', 1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_frequency(alist):\n",
    "    dict = {}\n",
    "    for item in alist:\n",
    "        if not item in dict:\n",
    "            dict[item] = alist.count(item)\n",
    "    sorted_items = sorted(dict.items(),key=lambda x: x[1],reverse=True)\n",
    "    return sorted_items\n",
    "\n",
    "word_frequency(['a','b','c','c','c','a'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Statistics\n",
    "Once you are able to determine the sentiment of an individual string of text (e.g., a single tweet's content), you can analyze an entire set of tweets from the user's timeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function (e.g., `analyze_tweets()`) that takes as an argument a **list** of tweet data (with the same structure as the imported `SAMPLE_TWEETS` variable), and _returns_ the data of interest to display in a table like the one at the very top of the notebook. In particular, you'll need to produce the following information **for each emotion**:\n",
    "\n",
    "1. The percentage of words _across all tweets_ that have that emotion\n",
    "2. The most common words _across all tweets_ that have that emotion (in order!)\n",
    "3. The most common **hashtags** _across all tweets_ associated with that emotion (see below)\n",
    "\n",
    "(Think carefully: should this data be stored in a _list_ or a _dictionary_?)\n",
    "\n",
    "Some tips for this task:\n",
    "\n",
    "- You can optionally create some \"helper\" functions to break up this task even further; define those functions in the same notebook cell or add additional cells.\n",
    "\n",
    "- You'll need to use your previous functions to get the _list of words_ and _dictionary of emotional words_ for each tweet. I recommend you assign the results of these methods as **new keys** of the respective tweet dictionary (so your tweet would gain a `words` key, for example).\n",
    "\n",
    "- In order to get the percentage of emotional words, divide the number of words that have that emotion by the total number of words _across all the tweets_. Counting how many total words are in the tweet set is a **reducing** operation: you should use the `reduce()` function for this.\n",
    "\n",
    "- For each emotion, you'll need to get a list of the words _across all the tweets_ that have that emotion (in order to determine how many there are for the percentage, as well as which are most common). This is another **reducing** operation; you should use the `reduce()` function to _add up_ all of these words (alternatively, the `sum()` function can be used here).\n",
    "\n",
    "- For emotion emotion, you will also need to calculate the most common [hashtags](https://en.wikipedia.org/wiki/Hashtag) for tweets that have _at least one word with that emotion_.\n",
    "\n",
    "    The Twitter data for each tweet includes a _list_ containing the hashtags found in that tweet&mdash;you should **NOT** try and search the tweet text for `#` symbols. These hashtags can be found in the `['entities']['hashtags'][i]['text']` element of each tweet&mdash;that is, the `'text'` key from _each_ element in the _list_ of the `'hashtags'` key in the `'entities'` dictionary of the tweet. See the `uw_school.json` example file to see this structure more clearly.\n",
    "\n",
    "    (You might use a _list comprehension_ to \"flatten\" this complex nesting structure into just a list of hashtag words).\n",
    "\n",
    "    Since hashtags are just words, you can use your function for finding the most common words to find the most common hashtags!\n",
    "\n",
    "You can test your function by passing in the `SAMPLE_TWEETS` variable as an argument and checking if your returned data has the same numbers as in the table at the top of the page. Note that only the first 3 most common words are listed (and may be in a different order in the case of ties)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('positive',\n",
       "  [0.061606160616061605,\n",
       "   'learn, faculty, happy',\n",
       "   '#accesstoinfoday, #indigenouspeoplesday, #idealistfair']),\n",
       " ('trust',\n",
       "  [0.030803080308030802,\n",
       "   'school, faculty, happy',\n",
       "   '#indigenouspeoplesday, #diversity']),\n",
       " ('anticipation',\n",
       "  [0.025302530253025302,\n",
       "   'happy, top, ready',\n",
       "   '#indigenouspeoplesday, #informatics, #info340']),\n",
       " ('joy',\n",
       "  [0.0176017601760176,\n",
       "   'happy, peace, deal',\n",
       "   '#indigenouspeoplesday, #accesstoinfoday']),\n",
       " ('surprise',\n",
       "  [0.009900990099009901,\n",
       "   'deal, award, surprised',\n",
       "   '#suzzallolibrary, #nobrainer']),\n",
       " ('negative', [0.0088008800880088, 'fall, rejection, outstanding', '']),\n",
       " ('sadness', [0.005500550055005501, 'fall, rejection, problem', '']),\n",
       " ('disgust', [0.0044004400440044, 'rejection, weird, finally', '']),\n",
       " ('fear', [0.0044004400440044, 'rejection, surprise, problem', '']),\n",
       " ('anger',\n",
       "  [0.0033003300330033004, 'rejection, disaster, involvement', '#mlis'])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def count_total(text_list):\n",
    "    whole_list = [text_to_word(item) for item in text_list]\n",
    "    return len(reduce(lambda x, y: x+y, whole_list))\n",
    "\n",
    "def emotions_in_hashtag(string,hashtag):\n",
    "    emodic = emotions_in_word('')\n",
    "    for emo in EMOTIONS:\n",
    "        emodic[emo] = word_to_emotion(string,emo)\n",
    "        if len(emodic[emo]) > 0:\n",
    "            emodic[emo] = hashtag\n",
    "    return emodic\n",
    "\n",
    "def get_table(text_list,hashtag_list):\n",
    "    ans = emotions_in_word('')\n",
    "    ans_hash = emotions_in_word('')\n",
    "    for i in range(len(text_list)):\n",
    "        dict_emotion = emotions_in_word(text_list[i])\n",
    "        dict_emotion_hashtag = emotions_in_hashtag(text_list[i],hashtag_list[i])\n",
    "        for emo in EMOTIONS:\n",
    "            ans[emo] += dict_emotion[emo]\n",
    "            ans_hash[emo] += dict_emotion_hashtag[emo]\n",
    "\n",
    "    dict_word_freq = emotions_in_word('')\n",
    "    dict_hash_freq = emotions_in_word('')\n",
    "    table_dict = {}\n",
    "    total_words = count_total(text_list)\n",
    "    for emo in EMOTIONS:\n",
    "        dict_word_freq[emo] = word_frequency(ans[emo])\n",
    "        dict_hash_freq[emo] = word_frequency(ans_hash[emo])      \n",
    "        eg_words = reduce(lambda x,y:x + ', ' + y,[dict_word_freq[emo][i][0] for i in range(len(dict_word_freq[emo])) if i < 3]) if dict_word_freq[emo]!=[] else ''\n",
    "        eg_hashtags = reduce(lambda x,y:x + ', ' + y,[dict_hash_freq[emo][i][0] for i in range(len(dict_hash_freq[emo])) if i < 3]) if dict_hash_freq[emo]!=[] else ''\n",
    "        # to solve the error of \"reduce() of empty sequence with no initial value\"\n",
    "        table_dict[emo] = reduce(lambda x,y:x + y,[i[1] for i in dict_word_freq[emo]]) if dict_word_freq[emo]!=[] else 0\n",
    "        table_dict[emo] = [table_dict[emo]/total_words,eg_words,eg_hashtags]\n",
    "    return table_dict\n",
    "\n",
    "\n",
    "def analyze_tweets(SAMPLE_TWEETS):\n",
    "    tweets_list = [tweet['text'] for tweet in SAMPLE_TWEETS]\n",
    "    hashtags = []\n",
    "    for i in SAMPLE_TWEETS:\n",
    "        temp = []\n",
    "        for j in i['entities']['hashtags']:\n",
    "                temp.append('#' + j['text'].lower())\n",
    "        hashtags.append(temp)\n",
    "    table_dict=get_table(tweets_list,hashtags)\n",
    "    sorted_table = sorted(table_dict.items(), key=lambda x: x[1],reverse=True)\n",
    "    return sorted_table\n",
    "analyze_tweets(SAMPLE_TWEETS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've analyzed the tweets, you will need to _display_ that information as a printed table (as in the example at the top of the page).\n",
    "\n",
    "Define another function to display this table (your function should take as an argument the data structure returned from your \"analysis\" function).\n",
    "\n",
    "This function will need to print out the table. Using the [string formatting](https://docs.python.org/3/library/string.html#format-examples) language (via the **`.format()`** string method) makes it possible to have equally sized \"columns\" of data. For more example, [this tutorial](https://www.digitalocean.com/community/tutorials/how-to-use-string-formatters-in-python-3) is pretty good (check out the \"Padding Variable Substitutions\" section).\n",
    "\n",
    "\n",
    "A few notes about formatting this output:\n",
    "\n",
    "- For your reference, the example table at the top of the page uses `14` characters for the first column, `11` characters for the second,  `35` for the third, and the \"remainder\" for the fourth. You are not required to match these numbers.\n",
    "\n",
    "- The percentage should be formatted with two decimals of precision (e.g., `1.23%`).\n",
    "\n",
    "- Both the example sentiment words and the hashtags should be outputted as a _comma-separated list_ with spaces between them (and no square brackets). The `join()` string method is good for converting lists to formatted strings. Both lists should also be limited to the 3 most common items.\n",
    "\n",
    "- Make sure to include `#` in front of the hashtags!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMOTION       % WORDS  EXAMPLE WORDS                      HASHTAGS                                \n",
      "positive      6.16%    learn, faculty, happy              #accesstoinfoday, #indigenouspeoplesday, #idealistfair\n",
      "trust         3.08%    school, faculty, happy             #indigenouspeoplesday, #diversity       \n",
      "anticipation  2.53%    happy, top, ready                  #indigenouspeoplesday, #informatics, #info340\n",
      "joy           1.76%    happy, peace, deal                 #indigenouspeoplesday, #accesstoinfoday \n",
      "surprise      0.99%    deal, award, surprised             #suzzallolibrary, #nobrainer            \n",
      "negative      0.88%    fall, rejection, outstanding                                               \n",
      "sadness       0.55%    fall, rejection, problem                                                   \n",
      "disgust       0.44%    rejection, weird, finally                                                  \n",
      "fear          0.44%    rejection, surprise, problem                                               \n",
      "anger         0.33%    rejection, disaster, involvement   #mlis                                   \n"
     ]
    }
   ],
   "source": [
    "def print_table(tweetlist):\n",
    "    print(\"{:13s} {:8s} {:34} {:40s}\".format('EMOTION','% WORDS','EXAMPLE WORDS','HASHTAGS'))\n",
    "    for tweet in tweetlist:\n",
    "        print(\"{:12s} {:5.2f}%    {:34} {:40s}\".format(tweet[0],tweet[1][0]*100,tweet[1][1],tweet[1][2]))\n",
    "print_table(analyze_tweets(SAMPLE_TWEETS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Live Data\n",
    "This is all good and well, but the real payoff would be to be able to see the sentiments of tweets taken directly from the Twitter feed of real users!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define _another_ function that takes in a Twitter username as an argument and then returns a list of dictionaries representing the tweets made by that user.\n",
    "\n",
    "Normally you would fetch this data by sending a request directly to the web service's API (e.g., to the the [statuses/user_timeline](https://developer.twitter.com/en/docs/tweets/timelines/api-reference/get-statuses-user_timeline) endpoint provided by the Twitter API at `https://api.twitter.com/1.1/statuses/user_timeline`). However, Twitter includes access controls so that only registered developers are allowed to send requests. While it is possible to register as a developer and access Twitter [directly through Python](https://python-twitter.readthedocs.io/en/latest/), this adds an extra level of complexity to the assignment.\n",
    "\n",
    "Instead, I've set up a [proxy](https://en.wikipedia.org/wiki/Proxy) that has all the access keys specified which you can use to search Twitter. This proxy is available at:\n",
    "\n",
    "**<https://faculty.washington.edu/joelross/proxy/twitter/timeline/>**\n",
    "\n",
    "Send a request to _that_ url instead of `https://api.twitter.com/1.1/statuses/user_timeline`, and it will redirect your request with the proper authentication to Twitter, and then give you back whatever JSON Twitter's API responded with.\n",
    "\n",
    "- You specify the same request parameters as you would when accessing Twitter directly. The request takes a `screen_name` request parameter which you can assign the given username. You can also specify the `count` parameter if you want to get more results back (up to 200); see the [documentation](https://developer.twitter.com/en/docs/tweets/timelines/api-reference/get-statuses-user_timeline) for details and other options you are welcome to use (just document them in your function's **docstring**).\n",
    "\n",
    "- **WARNING:** The proxy I have set up is **rate-limited** so that it can only accept 900 requests every 15 minutes. If all 40 students are working rapidly on the assignment at the same time, you may find yourself needing to wait a few minutes and try again. You are alternatively welcome to set up your own developer account and API keys; just make sure you don't put the keys under version control and upload them to GitHub!\n",
    "\n",
    "You can download the timeline data from Twitter using the [requests](http://docs.python-requests.org/en/master/user/quickstart/) module discussed in class: send a `GET` request to the [statuses/user_timeline](https://developer.twitter.com/en/docs/tweets/timelines/api-reference/get-statuses-user_timeline) endpoint provided by the Twitter API, and then use the `.json()` method to extract the JSON response as a Python _list_ or _dictionary_ value you can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def download_tweets(UserName,count):\n",
    "    response = requests.get('https://faculty.washington.edu/joelross/proxy/twitter/timeline?screen_name='+UserName+'&count='+str(count))\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define one last \"main\" function that will [prompt the user](https://docs.python.org/3/library/functions.html#input) for a Twitter username. The function should then call your \"download\" function to fetch the tweets, and pass the returned tweet data into your \"analyze\" and \"show\" functions in order to display your sentiment analysis of the user's timeline. \n",
    "\n",
    "**ADDITIONALLY**, `if` the user specifies `SAMPLE` (all caps) as the username, the function should instead show the analysis for the `SAMPLE_TWEETS` (this will help us out with grading)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    UserName = input('Please input user name: ')\n",
    "    if UserName =='SAMPLE':\n",
    "        print_table(analyze_tweets(SAMPLE_TWEETS))\n",
    "    else:\n",
    "        count = input('Please input number of tweets to be analyzed: ')\n",
    "        tweets = download_tweets(UserName,count)\n",
    "        print_table(analyze_tweets(tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your main function to try analyzing the timelines of different users and comparing their results. Are the current sentiments of the [iSchool](https://twitter.com/uw_ischool) and [CSE](https://twitter.com/uwcse) different in interesting ways?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMOTION       % WORDS  EXAMPLE WORDS                      HASHTAGS                                \n",
      "positive      6.67%    library, learn, technology         #library, #mlis, #mlw2018               \n",
      "trust         2.94%    excited, award, professor          #mlis, #library, #chi2018               \n",
      "anticipation  2.09%    excited, award, mobile             #mlis, #chi2018, #netinclusion          \n",
      "joy           1.57%    excited, award, special            #mlis, #chi2018, #internethealth        \n",
      "negative      1.06%    dangerous, violence, fear          #innovation, #datascientists, #facebookdatabreach\n",
      "surprise      0.98%    excited, award, trip               #chi2018, #netinclusion, #datascience   \n",
      "fear          0.88%    dangerous, violence, fear          #netinclusion, #facebook, #datafordecisionmaking\n",
      "sadness       0.67%    violence, poverty, winning         #chi2018, #mlis, #uwcherryblossoms      \n",
      "anger         0.62%    violence, fear, poverty            #datascientists, #netinclusion, #code   \n",
      "disgust       0.41%    poverty, winning, hate             #chi2018, #mlis, #machinelearning       \n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMOTION       % WORDS  EXAMPLE WORDS                      HASHTAGS                                \n",
      "positive      5.74%    professor, award, join             #uwallen, #memoriesindna, #ai           \n",
      "trust         2.98%    team, professor, award             #uwallen, #accessibility, #icymi        \n",
      "anticipation  2.83%    award, time, store                 #uwallen, #memoriesindna, #ghc18        \n",
      "joy           1.68%    award, favorite, good              #uwallen, #memoriesindna, #dnaday       \n",
      "surprise      0.98%    award, good, catch                 #uwallen, #memoriesindna, #chi2018      \n",
      "negative      0.90%    competition, split, weird          #uwallen, #chi2018, #augmentedreality   \n",
      "fear          0.38%    watch, fire, anxiety               #uwallen, #nsffunded, #icymi            \n",
      "sadness       0.38%    interested, fall, leave            #uwallen, #accesscomputing, #ictd       \n",
      "anger         0.25%    anxiety, defense, barrier          #uwallen, #sigcse2018                   \n",
      "disgust       0.23%    interested, weird, powerful        #augmentedreality, #accesscomputing, #ictd\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMOTION       % WORDS  EXAMPLE WORDS                      HASHTAGS                                \n",
      "positive      6.16%    learn, faculty, happy              #accesstoinfoday, #indigenouspeoplesday, #idealistfair\n",
      "trust         3.08%    school, faculty, happy             #indigenouspeoplesday, #diversity       \n",
      "anticipation  2.53%    happy, top, ready                  #indigenouspeoplesday, #informatics, #info340\n",
      "joy           1.76%    happy, peace, deal                 #indigenouspeoplesday, #accesstoinfoday \n",
      "surprise      0.99%    deal, award, surprised             #suzzallolibrary, #nobrainer            \n",
      "negative      0.88%    fall, rejection, outstanding                                               \n",
      "sadness       0.55%    fall, rejection, problem                                                   \n",
      "disgust       0.44%    rejection, weird, finally                                                  \n",
      "fear          0.44%    rejection, surprise, problem                                               \n",
      "anger         0.33%    rejection, disaster, involvement   #mlis                                   \n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMOTION       % WORDS  EXAMPLE WORDS                      HASHTAGS                                \n",
      "positive      4.97%    music, love, top                   #hot100, #snl, #metgala                 \n",
      "joy           2.80%    music, love, festival              #snl, #jazzfest, #metgala               \n",
      "anticipation  2.63%    watch, top, festival               #hot100, #snl, #bbmas                   \n",
      "trust         2.40%    top, chart, show                   #hot100, #metgala, #americanidol        \n",
      "negative      2.06%    childish, cry, bad                 #metgala, #bbmas, #hot100               \n",
      "fear          1.37%    watch, bad, savage                 #snl, #hot100, #youshouldknow           \n",
      "sadness       1.31%    music, sing, cry                   #snl, #youshouldknow, #thisisamerica    \n",
      "surprise      1.11%    festival, award, birthday          #snl, #metgala, #hot100                 \n",
      "anger         0.94%    bad, hot, savage                   #youshouldknow, #thisisamerica, #sinpijama\n",
      "disgust       0.54%    bad, celebrity, weird              #hot100, #thisisamerica                 \n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "84px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
